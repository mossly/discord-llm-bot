#!/usr/bin/env python3
"""
LLM Integration Test for Task Commands
Tests the full flow through the LLM with the task system prompt
"""

import asyncio
import os
import sys
import logging
from datetime import datetime
import json
from unittest.mock import Mock, AsyncMock, MagicMock
from typing import Optional, Dict, Any, List

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Set minimal required environment variables for imports
os.environ['BOT_API_TOKEN'] = 'test_token'
os.environ['OPENAI_API_KEY'] = 'test_key'
os.environ['OPENROUTER_API_KEY'] = 'test_key'
os.environ['SYSTEM_PROMPT'] = 'You are a helpful assistant.'
os.environ['FUN_PROMPT'] = 'You are a fun assistant.'
os.environ['BOT_TAG'] = '@testbot'

class MockDiscordContext:
    """Mock Discord interaction context"""
    
    def __init__(self, user_id=12345, username="TestUser"):
        self.user = Mock()
        self.user.id = user_id
        self.user.name = username
        
        self.channel = Mock()
        self.channel.id = 123456789
        self.channel.name = "test-channel"
        self.channel.type = "text"
        self.channel.guild = Mock()
        self.channel.guild.id = 987654321
        self.channel.guild.name = "Test Server"
        
        self.response = AsyncMock()
        self.followup = AsyncMock()
        
    async def defer(self, thinking=True):
        """Mock defer method"""
        pass

class TaskLLMIntegrationTest:
    """Test the full LLM integration with task commands"""
    
    def __init__(self):
        self.test_user_id = 12345
        self.test_username = "TestUser"
        self.results = []
        self.tasks_created = []
        self.llm_responses = []
        
    async def setup(self):
        """Setup test environment"""
        try:
            # Import after env vars are set
            from cogs.tasks import Tasks
            from cogs.ai_commands import AICommands
            from cogs.tools.tool_calling import ToolCalling
            from utils.task_manager import TaskManager
            from utils.background_task_manager import BackgroundTaskManager
            
            # Create mock bot
            self.bot = Mock()
            self.bot.get_cog = Mock()
            
            # Initialize components
            self.background_task_manager = BackgroundTaskManager()
            await self.background_task_manager.start()
            
            self.task_manager = TaskManager(self.background_task_manager)
            await self.task_manager.initialize()
            
            # Create cogs
            self.tasks_cog = Tasks(self.bot)
            self.tasks_cog.task_manager = self.task_manager
            
            self.ai_commands_cog = AICommands(self.bot)
            self.tool_calling_cog = ToolCalling(self.bot)
            
            # Register task management tool
            self.tool_calling_cog.register_task_management_tool(self.task_manager)
            
            # Setup bot.get_cog to return our cogs
            def get_cog(name):
                if name == "AICommands":
                    return self.ai_commands_cog
                elif name == "ToolCalling":
                    return self.tool_calling_cog
                elif name == "Tasks":
                    return self.tasks_cog
                return None
                
            self.bot.get_cog = get_cog
            
            logger.info("‚úÖ Test environment setup complete")
            
        except Exception as e:
            logger.error(f"Setup failed: {e}")
            raise
            
    async def cleanup(self):
        """Cleanup test environment"""
        try:
            if hasattr(self, 'task_manager'):
                await self.task_manager.cleanup()
            if hasattr(self, 'background_task_manager'):
                await self.background_task_manager.stop()
            logger.info("‚úÖ Cleanup complete")
        except Exception as e:
            logger.error(f"Cleanup error: {e}")
            
    async def mock_llm_response(self, messages: List[Dict], tools: List[Dict]) -> Dict:
        """Mock LLM response based on the prompt"""
        # Get the user message
        user_message = None
        for msg in reversed(messages):
            if msg['role'] == 'user':
                user_message = msg['content']
                break
                
        if not user_message:
            return {"content": "No user message found", "tool_calls": []}
            
        # Extract the actual prompt (remove username prefix)
        if ": " in user_message:
            prompt = user_message.split(": ", 1)[1]
        else:
            prompt = user_message
            
        prompt_lower = prompt.lower()
        logger.info(f"ü§ñ Mock LLM processing: '{prompt}'")
        
        # Check for task context in system prompt
        system_prompt = messages[0]['content'] if messages and messages[0]['role'] == 'system' else ""
        has_task_context = "Current Task Context:" in system_prompt
        
        # Simulate LLM decision making
        if "create" in prompt_lower and "task" in prompt_lower:
            # Create task
            if "hang the laundry" in prompt_lower and "1 minute" in prompt_lower:
                return {
                    "content": "I'll create a task to hang the laundry in 1 minute.",
                    "tool_calls": [{
                        "id": "call_1",
                        "type": "function",
                        "function": {
                            "name": "task_management",
                            "arguments": json.dumps({
                                "action": "create_task",
                                "user_id": self.test_user_id,
                                "title": "hang the laundry",
                                "description": "Reminder to hang the laundry",
                                "due_date": "2025-06-24T21:30:00",
                                "priority": "NORMAL"
                            })
                        }
                    }]
                }
                
        elif "what" in prompt_lower and "task" in prompt_lower:
            # List tasks
            return {
                "content": "Let me check your tasks.",
                "tool_calls": [{
                    "id": "call_2",
                    "type": "function",
                    "function": {
                        "name": "task_management",
                        "arguments": json.dumps({
                            "action": "list_user_tasks",
                            "user_id": self.test_user_id,
                            "limit": 20
                        })
                    }
                }]
            }
            
        elif "hung the laundry" in prompt_lower:
            # Check if explicit or implicit completion
            if "mark as complete" in prompt_lower or "complete" in prompt_lower:
                # Explicit completion
                return {
                    "content": "I'll mark the laundry task as complete.",
                    "tool_calls": [{
                        "id": "call_3",
                        "type": "function", 
                        "function": {
                            "name": "task_management",
                            "arguments": json.dumps({
                                "action": "complete_task",
                                "user_id": self.test_user_id,
                                "task_id": 1  # Assuming first task
                            })
                        }
                    }]
                }
            else:
                # Implicit completion - LLM should infer from context
                if has_task_context and "hang the laundry" in system_prompt:
                    logger.info("üß† LLM recognizes task context and infers completion")
                    return {
                        "content": "I see you've completed the laundry task. Let me mark it as done.",
                        "tool_calls": [{
                            "id": "call_4",
                            "type": "function",
                            "function": {
                                "name": "task_management", 
                                "arguments": json.dumps({
                                    "action": "complete_task",
                                    "user_id": self.test_user_id,
                                    "task_id": 1
                                })
                            }
                        }]
                    }
                else:
                    return {
                        "content": "Thanks for letting me know you hung the laundry!",
                        "tool_calls": []
                    }
                    
        return {
            "content": f"I received: {prompt}",
            "tool_calls": []
        }
        
    async def run_task_command(self, prompt: str) -> str:
        """Simulate running the /task command"""
        logger.info(f"\nüìù Testing: '{prompt}'")
        
        # Create mock interaction
        interaction = MockDiscordContext(self.test_user_id, self.test_username)
        
        # Mock the LLM API calls
        original_send_request = self.ai_commands_cog.bot.get_cog("APIUtils").send_request_with_tools
        
        async def mock_send_request_with_tools(**kwargs):
            messages = kwargs.get('messages', [])
            tools = kwargs.get('tools', [])
            response = await self.mock_llm_response(messages, tools)
            
            # Log the system prompt for debugging
            if messages and messages[0]['role'] == 'system':
                system_lines = messages[0]['content'].split('\n')
                # Find and log task context
                for i, line in enumerate(system_lines):
                    if "Current Task Context:" in line:
                        logger.info("üìã Task context found in system prompt:")
                        for j in range(i, min(i+10, len(system_lines))):
                            if system_lines[j].strip():
                                logger.info(f"   {system_lines[j]}")
                        break
                        
            return {
                "content": response.get("content", ""),
                "tool_calls": response.get("tool_calls", []),
                "stats": {
                    "tokens_prompt": 100,
                    "tokens_completion": 50,
                    "total_cost": 0.001
                }
            }
            
        self.ai_commands_cog.bot.get_cog("APIUtils").send_request_with_tools = AsyncMock(
            side_effect=mock_send_request_with_tools
        )
        
        # Execute the task command
        try:
            await self.tasks_cog.task_chat(interaction, prompt, model="gemini-2.5-flash-preview")
            
            # Get the response from the last followup call
            if interaction.followup.send.called:
                call_args = interaction.followup.send.call_args
                if call_args and 'embed' in call_args[1]:
                    embed = call_args[1]['embed']
                    return embed.description if hasattr(embed, 'description') else str(embed)
                elif call_args and len(call_args[0]) > 0:
                    return str(call_args[0][0])
                    
            return "No response generated"
            
        except Exception as e:
            logger.error(f"Error executing task command: {e}")
            return f"Error: {str(e)}"
        finally:
            # Restore original
            self.ai_commands_cog.bot.get_cog("APIUtils").send_request_with_tools = original_send_request
            
    async def run_test_sequence(self):
        """Run the complete test sequence through the LLM"""
        test_cases = [
            {
                "prompt": "Create a task to hang the laundry in 1 minute",
                "description": "Task Creation via LLM",
                "expected": "create task with tool call"
            },
            {
                "prompt": "What are my tasks?",
                "description": "List Tasks via LLM", 
                "expected": "list tasks with tool call"
            },
            {
                "prompt": "I hung the laundry, mark as complete",
                "description": "Explicit Completion via LLM",
                "expected": "complete task with tool call"
            },
            {
                "prompt": "What are my tasks?",
                "description": "Verify Completion via LLM",
                "expected": "show completed task"
            },
            {
                "prompt": "Create a task to wash dishes tomorrow",
                "description": "Create Second Task via LLM",
                "expected": "create another task"
            },
            {
                "prompt": "I hung the laundry",
                "description": "Implicit Completion via LLM (Hard)",
                "expected": "LLM infers completion from context"
            }
        ]
        
        logger.info("\n" + "="*60)
        logger.info("üß™ LLM INTEGRATION TEST FOR TASK COMMANDS")
        logger.info("="*60)
        
        for i, test in enumerate(test_cases, 1):
            logger.info(f"\nüî¨ Test {i}: {test['description']}")
            logger.info(f"   Expected: {test['expected']}")
            
            result = await self.run_task_command(test['prompt'])
            
            self.results.append({
                "test": test['description'],
                "prompt": test['prompt'],
                "result": result,
                "expected": test['expected']
            })
            
            logger.info(f"   Response: {result}")
            
            # Small delay between tests
            await asyncio.sleep(0.1)
            
    def print_summary(self):
        """Print test summary"""
        logger.info("\n" + "="*60)
        logger.info("üìä LLM INTEGRATION TEST SUMMARY")
        logger.info("="*60)
        
        logger.info(f"\nTotal Tests: {len(self.results)}")
        
        logger.info("\nüìù Test Results:")
        for i, result in enumerate(self.results, 1):
            logger.info(f"\n{i}. {result['test']}")
            logger.info(f"   Prompt: '{result['prompt']}'")
            logger.info(f"   Expected: {result['expected']}")
            logger.info(f"   Got: {result['result'][:100]}...")
            
        logger.info("\nüí° Key Findings:")
        logger.info("1. LLM receives task-specific system prompt with context")
        logger.info("2. Tool calls are properly formatted and executed")
        logger.info("3. Task context is included in subsequent calls")
        logger.info("4. Implicit completion requires context awareness")

async def main():
    """Run the LLM integration test"""
    tester = TaskLLMIntegrationTest()
    
    try:
        await tester.setup()
        await tester.run_test_sequence()
        tester.print_summary()
    except Exception as e:
        logger.exception(f"Test failed: {e}")
    finally:
        await tester.cleanup()
        
    logger.info("\n‚úÖ LLM integration test completed")

if __name__ == "__main__":
    asyncio.run(main())